<head>
    <title>Research | HKU-Shanghai ICRC</title>

    <link rel="icon" type="image/png" href="/assets/icon/hku-suqare.png">

    <link href="/ui2024/css/header.css" rel="stylesheet" type="text/css" media="all">
    <link href="/ui2024/css/footer.css" rel="stylesheet" type="text/css" media="all">
    <link href="/ui2024/css/format.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/font.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/banner.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/team.css" rel="stylesheet" type="text/css" media="all"/>

    <!-- <script src="/ui2024/js/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script> -->
    <script src="/ui2024/js/jquery-3.7.1.min.js" crossorigin="anonymous"></script>
</head>



<script>
    $.get("/ui2024/components/header.html",function(result){
        $("body").prepend(result)
    })
    $.get("/ui2024/components/footer.html",function(result){
        $("body").append(result)
    })
</script>
<style>
    body {
      font-family: Arial, sans-serif;
    }
    .directory {
      margin: 20px;
    }
    .directory h2 {
      color: #333;
      margin: 10px 0;
      cursor: pointer;
    }
    .directory ul {
      list-style-type: none;
      padding: 0;
    }
    .directory li {
      margin-bottom: 5px;
    }
    .subdirectory {
      margin-left: 20px;
    }
    .subdirectory li {
      margin-bottom: 2px;
    }
    .section {
      margin: 20px;
      padding: 10px;
      border: 1px solid #ccc;
    }
  </style>




<body>
    <div class="banner">
        <div class="banner_container">
            <div class="banner_title_en">
                <h1 class="Owhite">Research 学术研究</h1>
            </div>
        </div>
    </div>



    <br><br><br>


<!-- ########## White-Box ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">White-Box Transformers via Sparse Rate Reduction</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="80%">
                    <div>
                        <img src="./files/white-box/1.png" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>                
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin D. Haeffele, Yi Ma</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference on Neural Information Processing Systems (NeurIPS 2023)</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2306.01129" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/abs/2306.01129" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank" style="color: blue;">Code</a></strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## Miniamlistic white-box ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Emergence of Segmentation with Minimalistic White-Box Transformers</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="80%">
                    <div>
                        <img src="./files/white-box/2.png" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                        <img src="./files/white-box/3.png" width="100%">
                    </div>
                </td>                
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Yaodong Yu, Tianzhe Chu, Shengbang Tong, Ziyang Wu, Druv Pai, Sam Buchanan, Yi Ma</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference on Parsimony and Learning (CPAL) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2308.16271" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/abs/2308.16271" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank" style="color: blue;">Code</a></strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Transformer-like models for vision tasks have recently proven effective for a wide range of downstream applications such as segmentation and detection. Previous works have shown that segmentation properties emerge in vision transformers (ViTs) trained using self-supervised methods such as DINO, but not in those trained on supervised classification tasks. In this study, we probe whether segmentation emerges in transformer-based models solely as a result of intricate self-supervised learning mechanisms, or if the same emergence can be achieved under much broader conditions through proper design of the model architecture. Through extensive experimental results, we demonstrate that when employing a white-box transformer-like architecture known as CRATE, whose design explicitly models and pursues low-dimensional structures in the data distribution, segmentation properties, at both the whole and parts levels, already emerge with a minimalistic supervised training recipe. Layer-wise finer-grained analysis reveals that the emergent properties strongly corroborate the designed mathematical functions of the white-box network. Our results suggest a path to design white-box foundation models that are simultaneously highly performant and mathematically fully interpretable.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->

<!-- ########## Masked White-Box ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Masked Completion via Structured Diffusion with White-Box Transformers</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="80%">
                    <div>
                        <img src="./files/white-box/4.png" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>                
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Druv Pai, Ziyang Wu, Sam Buchanan, Yaodong Yu, Yi Ma</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2404.02446" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/abs/2404.02446" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/Ma-Lab-Berkeley/CRATE" target="_blank" style="color: blue;">Code</a></strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving a deep transformer-like masked autoencoder architecture, called CRATE-MAE, in which the role of each layer is mathematically fully interpretable: they transform the data distribution to and from a structured representation. Extensive empirical evaluations confirm our analytical insights. CRATE-MAE demonstrates highly promising performance on large-scale imagery datasets while using only ~30% of the parameters compared to the standard masked autoencoder with the same model configuration. The representations learned by CRATE-MAE have explicit structure and also contain semantic meaning.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->

<!-- ########## RoboTwin ########## -->
<div id="" style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="45%">
                    <div>
                        <img src="./files/RoboTwin/main.jpg" width="100%">
                    </div>
                </td>  
                <td width="55%">
                    <div>
                        <img src="./files/RoboTwin/2.png" width="100%">
                    </div>
                </td>               
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Yao Mu<sup>*</sup>, Tianxing Chen<sup>*</sup>, Zanxin Chen<sup>*</sup>, Shijia Peng<sup>*</sup>, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, Lunkai Lin, Zhiqiang Xie, Mingyu Ding and Ping Luo</strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>European Conference on Computer Vision (ECCV) 2024 Workshop, <a style="color: red;">Best Paper Award</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://tianxingchen.github.io/G3Flow" target="_blank" style="color: blue;">Project Page</a> / <a href="https://arxiv.org/pdf/2411.18369" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/abs/2411.18369" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/TianxingChen/G3Flow" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Using the COBOT Magic platform, we have collected diverse data on tool usage, human-robot interaction, and mobile manipulation. We present a cost-effective approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented towards functionality.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## A Video is Worth 256 Bases ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="55%">
                    <div>
                        <img src="./files/A-Video-is-Worth-256-Bases/2.png" width="100%">
                        <img src="./files/A-Video-is-Worth-256-Bases/1.png" width="100%">
                    </div>
                </td>  
                <td width="55%">
                    <div>
                        <img src="./files/A-Video-is-Worth-256-Bases/3.png" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Maomao Li, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, Dong Xu</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>The Conference on Computer Vision and Pattern Recognition (CVPR) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://stem-inv.github.io/page/" target="_blank" style="color: blue;">Project Page</a> / <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_A_Video_is_Worth_256_Bases_Spatial-Temporal_Expectation-Maximization_Inversion_for_CVPR_2024_paper.pdf" target="_blank" style="color: blue;">Paper</a> / <a href="https://github.com/STEM-Inv/STEM-Inv" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">This paper presents a video inversion approach for zero-shot video editing, which models the input video with low-rank representation during the inversion process. The existing video editing methods usually apply the typical 2D DDIM inversion or naive spatial-temporal DDIM inversion before editing, which leverages time-varying representation for each frame to derive noisy latent. Unlike most existing approaches, we propose a Spatial-Temporal Expectation-Maximization (STEM) inversion, which formulates the dense video feature under an expectation-maximization manner and iteratively estimates a more compact basis set to represent the whole video. Each frame applies the fixed and global representation for inversion, which is more friendly for temporal consistency during reconstruction and editing. Extensive qualitative and quantitative experiments demonstrate that our STEM inversion can achieve consistent improvement on two state-of-the-art video editing methods.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## CO3 ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">CO<sup>3</sup>: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="60%">
                    <div>
                        <img src="./files/CO3/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="40%">
                    <div>
                        <img src="./files/CO3/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Runjian Chen, Yao Mu, Runsen Xu, Wenqi Shao, Chenhan Jiang, Hang Xu, Zhenguo Li, Ping Luo</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2023</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://openreview.net/forum?id=QUaDoIdgo0" target="_blank" style="color: blue;">Paper</a> / <a href="https://github.com/Runjian-Chen/CO3" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Unsupervised contrastive learning for indoor-scene point clouds has achieved great successes. However, unsupervised learning point clouds in outdoor scenes remains challenging because previous methods need to reconstruct the whole scene and capture partial views for the contrastive objective. This is infeasible in outdoor scenes with moving objects, obstacles, and sensors. In this paper, we propose CO^3, namely Cooperative Contrastive Learning and Contextual Shape Prediction, to learn 3D representation for outdoor-scene point clouds in an unsupervised manner.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->



<!-- ########## OmniQuant ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <img src="./files/omniquant/1.png" width="100%">
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <img src="./files/omniquant/2.png" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Wenqi Shao<sup>*</sup>, Mengzhao Chen<sup>*</sup>, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2024, <a style="color: red">Spotlight</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2308.13137" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenGVLab/OmniQuant" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We propose OmniQuant, a novel post-training quantization (PTQ) technique for large language models (LLMs) that enhances performance in diverse quantization settings, particularly in extremely low-bit quantization. It introduces two key innovations: Learnable Weight Clipping (LWC) to optimize weight clipping thresholds and Learnable Equivalent Transformation (LET) to handle activation outliers by shifting quantization challenges to weights. OmniQuant operates within a differentiable framework using block-wise error minimization, enabling efficient optimization for both weight-only and weight-activation quantization.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## CLOVER ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">CLOVER: Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation 
    </strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="55%">
                    <div>
                        <img src="./files/CLOVER/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="45%">
                    <div>
                        <img src="./files/CLOVER/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Qingwen Bu<sup>*</sup>, Jia Zeng<sup>*</sup>, Li Chen<sup>*</sup>, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, Hongyang Li</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference on Neural Information Processing Systems (NeurIPS 2024)</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2409.09016" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenDriveLab/CLOVER" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. </strong>
</div>
<br><br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>
<!-- ####################### -->


<!-- ########## RoboDual ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Towards Synergistic, Generalized and Efficient Dual-System for Robotic Manipulation
    </strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="46%">
                    <div>
                        <img src="./files/RoboDual/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="54%">
                    <div>
                        <img src="./files/RoboDual/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, Yu Qiao</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Preprint</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2410.08001" target="_blank" style="color: blue;">arXiv</a> / <a href="https://opendrivelab.com/RoboDual/" target="_blank" style="color: blue;">Page</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language- action (VLA) based generalist.
    </strong>
</div>
<br><br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>
<!-- ####################### -->


<!-- ########## DriveLM ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">DriveLM: Driving with Graph Visual Question Answering</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="80%">
                    <div>
                        <img src="./files/DriveLM/1.jpg" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>               
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Chonghao Sima<sup>*</sup>, Katrin Renz<sup>*</sup>, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, Hongyang Li</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>European Conference on Computer Vision (ECCV) 2024, <a style="color: red">Oral</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://opendrivelab.com/DriveLM/" target="_blank" style="color: blue;">Project Page</a> / <a href="https://link.springer.com/chapter/10.1007/978-3-031-72943-0_15" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/pdf/2312.14150v1" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenDriveLab/DriveLM?tab=readme-ov-file" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We explore how vision-language models (VLMs) trained on web-scale data can enhance generalization and interactivity in end-to-end driving systems. Unlike recent single-round VQA approaches, human drivers reason in multiple steps, starting from object localization to estimating interactions and planning actions. To mimic this process, we propose Graph VQA, a task that models graph-structured reasoning across perception, prediction, and planning. We introduce DriveLM-Data, a dataset based on nuScenes and CARLA, and a VLM-based baseline, DriveLM-Agent, for jointly addressing Graph VQA and autonomous driving. Our work aims to advance the integration of VLMs into driving systems and provides publicly available resources to support future research.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->




<!-- ########## An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <img src="./files/An-In-depth-Investigation/3.jpg" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <img src="./files/An-In-depth-Investigation/4.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Yunzhe Hu, Difan Zou, Dong Xu</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference on Neural Information Processing Systems (NeurIPS) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2411.17182" target="_blank" style="color: blue;">arXiv</a></strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We investigate the properties and limitations of a recently proposed Transformer-like model, Coding Rate Reduction Transformer (CRATE), that is designed by unrolling optimization on Sparse Rate Reduction (SRR) and believed to be interpretable by construction. We also unveil the causal relationship between SRR and the generalization of this model family. Our findings reveal that SRR indeed has a postive and relatively strong correlation to generalization, and can be incorporated as training regularization. However, it is still far from being principled guidance to design better models. </strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->




<!-- ########## EUnet ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Learning 3D Garment Animation from Trajectories of A Piece of Cloth</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="100%">
                    <div>
                        <img src="./files/eunet/1.png" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>                
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Yidi Shao, Chen Change Loy, Bo Dai</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference on Neural Information Processing Systems (NeurIPS) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://openreview.net/pdf?id=yeFx5NQmr7" target="_blank" style="color: blue;">Project Page</a> /  <a href="https://github.com/ftbabi/EUNet_NeurIPS2024" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">In this paper, instead of using garment-wise supervised-learning we adopt a disentangled scheme to learn how to animate observed garments: 1). learning constitutive behaviors from the observed cloth; 2). dynamically animate various garments constrained by the learned constitutive laws. Specifically, we propose Energy Unit network (EUNet) to model the constitutive relations in the format of energy directly from the observed trajectories of piece of cloth. We further apply the pre-trained EUNet to animate various garments based on energy optimizations. The disentangled scheme alleviates the need of garment data and enables us to robustly animate garments.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## Template ########## -->

<!-- <div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Project Name</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <h2>Image / Video 1, see RoboTwin.</h2>
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <h2>Image / Video 2, see RoboTwin.</h2>
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;">Author List</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference / Journal</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="" target="_blank" style="color: blue;">Project Page</a> / <a href="" target="_blank" style="color: blue;">Paper</a> / <a href="" target="_blank" style="color: blue;">arXiv</a> / <a href="" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Content.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br> -->

<!-- ####################### -->





</body>






<!-- ####################### Orig ####################### -->

<!-- <div class="block_left" style="margin-top: 0; padding-top: 0">
    <div style="flex-grow: 1;">
        <div class="person_containe; width: 100%;">

            <h1>Project 1</h1>
            
            <h3>Description</h3>
            
            <p>Content</p>

        </div>
    </div> 
</div>
<br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br> -->