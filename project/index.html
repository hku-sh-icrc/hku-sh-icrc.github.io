<head>
    <title>Project | HKU-Shanghai ICRC</title>

    <link rel="icon" type="image/png" href="/assets/icon/hku-suqare.png">

    <link href="/ui2024/css/header.css" rel="stylesheet" type="text/css" media="all">
    <link href="/ui2024/css/footer.css" rel="stylesheet" type="text/css" media="all">
    <link href="/ui2024/css/format.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/font.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/banner.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/team.css" rel="stylesheet" type="text/css" media="all"/>

    <!-- <script src="/ui2024/js/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script> -->
    <script src="/ui2024/js/jquery-3.7.1.min.js" crossorigin="anonymous"></script>
</head>



<script>
    $.get("/ui2024/components/header.html",function(result){
        $("body").prepend(result)
    })
    $.get("/ui2024/components/footer.html",function(result){
        $("body").append(result)
    })
</script>



<body>
    <div class="banner">
        <div class="banner_container">
            <div class="banner_title_en">
                <h1 class="Owhite">Project</h1>
            </div>
        </div>
    </div>



    <br><br><br>
<!-- ########## RoboTwin ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="45%">
                    <div>
                        <img src="./files/RoboTwin/main.jpg" width="100%">
                    </div>
                </td>  
                <td width="55%">
                    <div>
                        <img src="./files/RoboTwin/2.png" width="100%">
                    </div>
                </td>               
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Yao Mu<sup>*</sup>, Tianxing Chen<sup>*</sup>, Zanxin Chen<sup>*</sup>, Shijia Peng<sup>*</sup>, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, Lunkai Lin, Zhiqiang Xie, Mingyu Ding and Ping Luo<sup>†</sup></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>European Conference on Computer Vision (ECCV) 2024 Workshop @ MAAS, <a style="color: red;">Best Paper Award (Early Version)</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://tianxingchen.github.io/G3Flow" target="_blank" style="color: blue;">Project Page</a> / <a href="https://arxiv.org/pdf/2411.18369" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/abs/2411.18369" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/TianxingChen/G3Flow" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Using the COBOT Magic platform, we have collected diverse data on tool usage, human-robot interaction, and mobile manipulation. We present a cost-effective approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented towards functionality.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## A Video is Worth 256 Bases ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="55%">
                    <div>
                        <img src="./files/A-Video-is-Worth-256-Bases/2.png" width="100%">
                        <img src="./files/A-Video-is-Worth-256-Bases/1.png" width="100%">
                    </div>
                </td>  
                <td width="55%">
                    <div>
                        <img src="./files/A-Video-is-Worth-256-Bases/3.png" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Maomao Li, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, Dong Xu<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>The Conference on Computer Vision and Pattern Recognition (CVPR) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://stem-inv.github.io/page/" target="_blank" style="color: blue;">Project Page</a> / <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_A_Video_is_Worth_256_Bases_Spatial-Temporal_Expectation-Maximization_Inversion_for_CVPR_2024_paper.pdf" target="_blank" style="color: blue;">Paper</a> / <a href="https://github.com/STEM-Inv/STEM-Inv" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">This paper presents a video inversion approach for zero-shot video editing, which models the input video with low-rank representation during the inversion process. The existing video editing methods usually apply the typical 2D DDIM inversion or naive spatial-temporal DDIM inversion before editing, which leverages time-varying representation for each frame to derive noisy latent. Unlike most existing approaches, we propose a Spatial-Temporal Expectation-Maximization (STEM) inversion, which formulates the dense video feature under an expectation-maximization manner and iteratively estimates a more compact basis set to represent the whole video. Each frame applies the fixed and global representation for inversion, which is more friendly for temporal consistency during reconstruction and editing. Extensive qualitative and quantitative experiments demonstrate that our STEM inversion can achieve consistent improvement on two state-of-the-art video editing methods.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## CO3 ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">CO<sup>3</sup>: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="60%">
                    <div>
                        <img src="./files/CO3/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="40%">
                    <div>
                        <img src="./files/CO3/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Runjian Chen, Yao Mu, Runsen Xu, Wenqi Shao, Chenhan Jiang, Hang Xu, Zhenguo Li, Ping Luo<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2023</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://openreview.net/forum?id=QUaDoIdgo0" target="_blank" style="color: blue;">Paper</a> / <a href="https://github.com/Runjian-Chen/CO3" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Unsupervised contrastive learning for indoor-scene point clouds has achieved great successes. However, unsupervised learning point clouds in outdoor scenes remains challenging because previous methods need to reconstruct the whole scene and capture partial views for the contrastive objective. This is infeasible in outdoor scenes with moving objects, obstacles, and sensors. In this paper, we propose CO^3, namely Cooperative Contrastive Learning and Contextual Shape Prediction, to learn 3D representation for outdoor-scene point clouds in an unsupervised manner.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->



<!-- ########## OmniQuant ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <img src="./files/omniquant/1.png" width="100%">
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <img src="./files/omniquant/2.png" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Wenqi Shao<sup>*</sup>, Mengzhao Chen<sup>*</sup>, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2024, <a style="color: red">Spotlight</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2308.13137" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenGVLab/OmniQuant" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We propose OmniQuant, a novel post-training quantization (PTQ) technique for large language models (LLMs) that enhances performance in diverse quantization settings, particularly in extremely low-bit quantization. It introduces two key innovations: Learnable Weight Clipping (LWC) to optimize weight clipping thresholds and Learnable Equivalent Transformation (LET) to handle activation outliers by shifting quantization challenges to weights. OmniQuant operates within a differentiable framework using block-wise error minimization, enabling efficient optimization for both weight-only and weight-activation quantization.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## CLOVER ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">CLOVER: Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation 
    </strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="55%">
                    <div>
                        <img src="./files/CLOVER/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="45%">
                    <div>
                        <img src="./files/CLOVER/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Qingwen Bu<sup>*</sup>, Jia Zeng<sup>*</sup>, Li Chen<sup>*</sup>, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, Hongyang Li</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>The Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS 2024)</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2409.09016" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenDriveLab/CLOVER" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. </strong>
</div>
<br><br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>
<!-- ####################### -->


<!-- ########## RoboDual ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Towards Synergistic, Generalized and Efficient Dual-System for Robotic Manipulation
    </strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="46%">
                    <div>
                        <img src="./files/RoboDual/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="54%">
                    <div>
                        <img src="./files/RoboDual/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, Yu Qiao</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Preprint</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2410.08001" target="_blank" style="color: blue;">arXiv</a> / <a href="https://opendrivelab.com/RoboDual/" target="_blank" style="color: blue;">Page</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language- action (VLA) based generalist.
    </strong>
</div>
<br><br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>
<!-- ####################### -->


<!-- ########## DriveLM ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">DriveLM: Driving with Graph Visual Question Answering</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="80%">
                    <div>
                        <img src="./files/DriveLM/1.jpg" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>               
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Chonghao Sima<sup>*</sup>, Katrin Renz<sup>*</sup>, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger<sup>†</sup>, Hongyang Li<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>European Conference on Computer Vision (ECCV) 2024, <a style="color: red">Oral</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://opendrivelab.com/DriveLM/" target="_blank" style="color: blue;">Project Page</a> / <a href="https://link.springer.com/chapter/10.1007/978-3-031-72943-0_15" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/pdf/2312.14150v1" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenDriveLab/DriveLM?tab=readme-ov-file" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We explore how vision-language models (VLMs) trained on web-scale data can enhance generalization and interactivity in end-to-end driving systems. Unlike recent single-round VQA approaches, human drivers reason in multiple steps, starting from object localization to estimating interactions and planning actions. To mimic this process, we propose Graph VQA, a task that models graph-structured reasoning across perception, prediction, and planning. We introduce DriveLM-Data, a dataset based on nuScenes and CARLA, and a VLM-based baseline, DriveLM-Agent, for jointly addressing Graph VQA and autonomous driving. Our work aims to advance the integration of VLMs into driving systems and provides publicly available resources to support future research.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->



<!-- ########## Task-Aware Scene Representations for Robotic Manipulation via Hypernetworks ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Task-Aware Scene Representations for Robotic Manipulation via Hypernetworks</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="65%">
                    <div>
                        <img src="./files/Task-Aware-Scene-Representations-for Robotic-Manipulation-via-Hypernetworks/1.jpg" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>  
                <td width="35%">
                    <div>
                        <img src="./files/Task-Aware-Scene-Representations-for Robotic-Manipulation-via-Hypernetworks/2.jpg" width="100%">
                        <!-- <h2>Image / Video 2, see RoboTwin.</h2> -->
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Li Sun, Jiefeng Wu, Yanchao Yang</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Under Review</i></strong>
    <div style="height: 20px;"></div>
    <!-- <strong style="color: black; font-size: 1.5vw;"><a href="" target="_blank" style="color: blue;">Project Page</a> / <a href="" target="_blank" style="color: blue;">Paper</a> / <a href="" target="_blank" style="color: blue;">arXiv</a> / <a href="" target="_blank" style="color: blue;">Code</a> </strong> -->
    <!-- <div style="height: 20px;"></div> -->
    <strong style="color: gray; font-size: 1.2vw;">he advancement of intelligent agents adept at interacting with dynamic physical environments necessitates improvements in policy learning pipelines. Conventional pipelines often separate scene representation and policy formulation, resulting in task-agnostic representations. In this work, we propose a task-aware scene representation framework that employs hypernetworks to adapt scene representations based on task-specific information. Our method incorporates task descriptions and progression specifics derived from observations into the representation extraction process by using hypernetworks to generate weights for the scene representation adaptation network. Through both qualitative analysis and quantitative results, we demonstrate that our task-aware representations significantly enhance the performance and adaptability of embodied agents in various settings.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <img src="./files/An-In-depth-Investigation/1.jpg" width="100%">
                        <!-- <h2>Image / Video 1, see RoboTwin.</h2> -->
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <img src="./files/An-In-depth-Investigation/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Yunzhe Hu, Difan Zou, Dong Xu</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference on Neural Information Processing Systems (NeurIPS) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2411.17182" target="_blank" style="color: blue;">arXiv</a></strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We investigate the properties and limitations of a recently proposed Transformer-like model, Coding Rate Reduction Transformer (CRATE), that is designed by unrolling optimization on Sparse Rate Reduction (SRR) and believed to be interpretable by construction. We also unveil the causal relationship between SRR and the generalization of this model family. Our findings reveal that SRR indeed has a postive and relatively strong correlation to generalization, and can be incorporated as training regularization. However, it is still far from being principled guidance to design better models. </strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->

<!-- ########## Template ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Project Name</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <!-- <img src="./files/RoboTwin/main.jpg" width="100%"> -->
                        <h2>Image / Video 1, see RoboTwin.</h2>
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <!-- <img src="./files/RoboTwin/main.jpg" width="100%"> -->
                        <h2>Image / Video 2, see RoboTwin.</h2>
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Author List</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference / Journal</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="" target="_blank" style="color: blue;">Project Page</a> / <a href="" target="_blank" style="color: blue;">Paper</a> / <a href="" target="_blank" style="color: blue;">arXiv</a> / <a href="" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Content.</strong>
</div>
<br><br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->





</body>






<!-- ####################### Orig ####################### -->

<!-- <div class="block_left" style="margin-top: 0; padding-top: 0">
    <div style="flex-grow: 1;">
        <div class="person_containe; width: 100%;">

            <h1>Project 1</h1>
            
            <h3>Description</h3>
            
            <p>Content</p>

        </div>
    </div> 
</div>
<br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br> -->