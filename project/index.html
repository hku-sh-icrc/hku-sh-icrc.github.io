<head>
    <title>Project | HKU-Shanghai ICRC</title>

    <link rel="icon" type="image/png" href="/assets/icon/hku-suqare.png">

    <link href="/ui2024/css/header.css" rel="stylesheet" type="text/css" media="all">
    <link href="/ui2024/css/footer.css" rel="stylesheet" type="text/css" media="all">
    <link href="/ui2024/css/format.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/font.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/banner.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/team.css" rel="stylesheet" type="text/css" media="all"/>

    <!-- <script src="/ui2024/js/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script> -->
    <script src="/ui2024/js/jquery-3.7.1.min.js" crossorigin="anonymous"></script>
</head>



<script>
    $.get("/ui2024/components/header.html",function(result){
        $("body").prepend(result)
    })
    $.get("/ui2024/components/footer.html",function(result){
        $("body").append(result)
    })
</script>



<body>
    <div class="banner">
        <div class="banner_container">
            <div class="banner_title_en">
                <h1 class="Owhite">Project</h1>
            </div>
        </div>
    </div>



    <br><br><br>
<!-- ########## RoboTwin ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td height="100%">
                    <div>
                        <img src="./files/RoboTwin/main.jpg" width="100%">
                    </div>
                </td>  
                <td height="100%">
                    <div>
                        <video autoplay loop muted width="100%" height="auto" 
                        x5-video-player-fullscreen="true"
                        x5-playsinline
                        playsinline
                        webkit-playsinline
                        >
                            <source src="./files/RoboTwin/RoboTwin.mp4" type="video/mp4">
                            video error
                          </video>
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Yao Mu<sup>*</sup>, Tianxing Chen<sup>*</sup>, Zanxin Chen<sup>*</sup>, Shijia Peng<sup>*</sup>, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, Lunkai Lin, Zhiqiang Xie, Mingyu Ding and Ping Luo<sup>†</sup></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>ECCV 2024 Workshop @ MAAS, <a style="color: red;">Best Paper Award (Early Version)</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://tianxingchen.github.io/G3Flow" target="_blank" style="color: blue;">Project Page</a> / <a href="https://arxiv.org/pdf/2411.18369" target="_blank" style="color: blue;">Paper</a> / <a href="https://arxiv.org/abs/2411.18369" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/TianxingChen/G3Flow" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Using the COBOT Magic platform, we have collected diverse data on tool usage, human-robot interaction, and mobile manipulation. We present a cost-effective approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented towards functionality.</strong>
</div>
<br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## A Video is Worth 256 Bases ########## -->
<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="55%">
                    <div>
                        <img src="./files/A-Video-is-Worth-256-Bases/2.png" width="100%">
                        <img src="./files/A-Video-is-Worth-256-Bases/1.png" width="100%">
                    </div>
                </td>  
                <td width="55%">
                    <div>
                        <img src="./files/A-Video-is-Worth-256-Bases/3.png" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Maomao Li, Yu Li, Tianyu Yang, Yunfei Liu, Dongxu Yue, Zhihui Lin, Dong Xu<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>The Conference on Computer Vision and Pattern Recognition (CVPR) 2024</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://stem-inv.github.io/page/" target="_blank" style="color: blue;">Project Page</a> / <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_A_Video_is_Worth_256_Bases_Spatial-Temporal_Expectation-Maximization_Inversion_for_CVPR_2024_paper.pdf" target="_blank" style="color: blue;">Paper</a> / <a href="https://github.com/STEM-Inv/STEM-Inv" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">This paper presents a video inversion approach for zero-shot video editing, which models the input video with low-rank representation during the inversion process. The existing video editing methods usually apply the typical 2D DDIM inversion or naive spatial-temporal DDIM inversion before editing, which leverages time-varying representation for each frame to derive noisy latent. Unlike most existing approaches, we propose a Spatial-Temporal Expectation-Maximization (STEM) inversion, which formulates the dense video feature under an expectation-maximization manner and iteratively estimates a more compact basis set to represent the whole video. Each frame applies the fixed and global representation for inversion, which is more friendly for temporal consistency during reconstruction and editing. Extensive qualitative and quantitative experiments demonstrate that our STEM inversion can achieve consistent improvement on two state-of-the-art video editing methods.</strong>
</div>
<br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->


<!-- ########## CO3 ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">CO<sup>3</sup>: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="60%">
                    <div>
                        <img src="./files/CO3/1.jpg" width="100%">
                    </div>
                </td>  
                <td width="40%">
                    <div>
                        <img src="./files/CO3/2.jpg" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Runjian Chen, Yao Mu, Runsen Xu, Wenqi Shao, Chenhan Jiang, Hang Xu, Zhenguo Li, Ping Luo<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2023</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://openreview.net/forum?id=QUaDoIdgo0" target="_blank" style="color: blue;">Paper</a> / <a href="https://github.com/Runjian-Chen/CO3" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Unsupervised contrastive learning for indoor-scene point clouds has achieved great successes. However, unsupervised learning point clouds in outdoor scenes remains challenging because previous methods need to reconstruct the whole scene and capture partial views for the contrastive objective. This is infeasible in outdoor scenes with moving objects, obstacles, and sensors. In this paper, we propose CO^3, namely Cooperative Contrastive Learning and Contextual Shape Prediction, to learn 3D representation for outdoor-scene point clouds in an unsupervised manner.</strong>
</div>
<br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->



<!-- ########## Template ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <img src="./files/omniquant/1.png" width="100%">
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <img src="./files/omniquant/2.png" width="100%">
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Wenqi Shao<sup>*</sup>, Mengzhao Chen<sup>*</sup>, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo<sup>†</sup></strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>International Conference on Learning Representation (ICLR) 2024, <a style="color: red">Spotlight</a></i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="https://arxiv.org/abs/2308.13137" target="_blank" style="color: blue;">arXiv</a> / <a href="https://github.com/OpenGVLab/OmniQuant" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">We propose OmniQuant, a novel post-training quantization (PTQ) technique for large language models (LLMs) that enhances performance in diverse quantization settings, particularly in extremely low-bit quantization. It introduces two key innovations: Learnable Weight Clipping (LWC) to optimize weight clipping thresholds and Learnable Equivalent Transformation (LET) to handle activation outliers by shifting quantization challenges to weights. OmniQuant operates within a differentiable framework using block-wise error minimization, enabling efficient optimization for both weight-only and weight-activation quantization.</strong>
</div>
<br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->




<!-- ########## Template ########## -->

<div style="padding-left: 15%; padding-right: 15%;">
    <strong style="color: black; font-size: 2.5vw;">Project Name</strong>
    <table width="100%" cellspacing='0'>
        <tbody>
            <tr style="width: 100%;">
                <td width="50%">
                    <div>
                        <!-- <img src="./files/RoboTwin/main.jpg" width="100%"> -->
                        <h2>Image / Video 1, see RoboTwin.</h2>
                    </div>
                </td>  
                <td width="50%">
                    <div>
                        <!-- <img src="./files/RoboTwin/main.jpg" width="100%"> -->
                        <h2>Image / Video 2, see RoboTwin.</h2>
                    </div>
                </td>                  
            </tr>   
        </tbody>
    </table>
    <strong style="color: black; font-size: 1.5vw;">Author List</strong> 
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><i>Conference / Journal</i></strong>
    <div style="height: 20px;"></div>
    <strong style="color: black; font-size: 1.5vw;"><a href="" target="_blank" style="color: blue;">Project Page</a> / <a href="" target="_blank" style="color: blue;">Paper</a> / <a href="" target="_blank" style="color: blue;">arXiv</a> / <a href="" target="_blank" style="color: blue;">Code</a> </strong>
    <div style="height: 20px;"></div>
    <strong style="color: gray; font-size: 1.2vw;">Content.</strong>
</div>
<br><br>

<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br>

<!-- ####################### -->





</body>






<!-- ####################### Orig ####################### -->

<!-- <div class="block_left" style="margin-top: 0; padding-top: 0">
    <div style="flex-grow: 1;">
        <div class="person_containe; width: 100%;">

            <h1>Project 1</h1>
            
            <h3>Description</h3>
            
            <p>Content</p>

        </div>
    </div> 
</div>
<br><br>
<div class="block" style="margin: 0;">
    <div class="navigator"></div>
</div>
<br><br><br> -->